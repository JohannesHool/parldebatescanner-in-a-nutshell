{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from PDFs, Generate Embeddings\n",
    "parldebatescanner.org uses the [opendata web services of parlament.ch](https://www.parlament.ch/de/%C3%BCber-das-parlament/fakten-und-zahlen/open-data-web-services) to retrieve speeches from sessions in the national council and council of states in the swiss parliament. For simplicity and based on the feedback of people interested in the code we do not use the parlament's API but extract text from PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import fitz # PyMuPDF used for text extraction from PDFs\n",
    "from sentence_transformers import SentenceTransformer # Sentence Transformer to generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model paraphrase-multilingual-MiniLM-L12-v2 can handle a context size of 128 tokens. For simplicity we use a rule of thumb and define the chunk size to be 4*128 characters as a token represents approximately 4 characters. A more sophisticated approach would be to use the sentence_transformer models encoder to split up the text to sizes of exactly 128 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default (relative) directories\n",
    "DATA_PATH = 'data'\n",
    "RAW_DATA_PATH = 'data/raw'\n",
    "TRANSFORMED_DATA_PATH = 'data/transformed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "if not os.path.exists(RAW_DATA_PATH):\n",
    "    os.makedirs(RAW_DATA_PATH)\n",
    "if not os.path.exists(TRANSFORMED_DATA_PATH):\n",
    "    os.makedirs(TRANSFORMED_DATA_PATH)\n",
    "\n",
    "MODEL = 'paraphrase-multilingual-MiniLM-L12-v2' # https://www.sbert.net/docs/pretrained_models.html\n",
    "CHUNK_SIZE = 128*4 # Number of characters that make up a text chunk\n",
    "OVERLAP = 1/5 # Percentage of Chunk size to overlap\n",
    "MIN_BLOCK_LENGTH = 120 # Minimum number of characters of a text to be deemed valuable\n",
    "\n",
    "model = SentenceTransformer(MODEL)\n",
    "output_file_name = 'dataset_1.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we simply extract all text blocks from the PDFs using PyMuPDF. A more sophisticated approach would be to combine and cleanse the blocks. There are also other PDF extraction libraries that e.g. use OCR. However, OCR is not in every case better than PyMuPDF especially if the PDF's structure is very simple.\n",
    "\n",
    "For simplicity we only remove some special characters from the text for cleansing. There are many more sophisticated approaches to cleanse the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to read: ['fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1-doc_12-de-pdf-a.pdf']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>block_index</th>\n",
       "      <th>block_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau bedankt s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3. Ihre elektronische Stellungnahme senden Sie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau begrüsst,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Die \"Generaleinwilligung\" ist kein Freibrief f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Erlangt die Prüfperson nach Abschluss des klin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_name  page_number  \\\n",
       "0  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            1   \n",
       "1  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            2   \n",
       "2  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            4   \n",
       "3  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            4   \n",
       "4  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            5   \n",
       "\n",
       "   block_index                                         block_text  \n",
       "0            0  Der Regierungsrat des Kantons Aargau bedankt s...  \n",
       "1            1  3. Ihre elektronische Stellungnahme senden Sie...  \n",
       "2            2  Der Regierungsrat des Kantons Aargau begrüsst,...  \n",
       "3            3  Die \"Generaleinwilligung\" ist kein Freibrief f...  \n",
       "4            4  Erlangt die Prüfperson nach Abschluss des klin...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all files in the folder RAW_DATA_PATH\n",
    "files_to_read = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "print('Files to read:', files_to_read)\n",
    "\n",
    "# Placeholder for extracted text blocks\n",
    "data = []\n",
    "\n",
    "# Iterate over all files to read\n",
    "for file_name in files_to_read:\n",
    "    \n",
    "    # Open the PDF using PyMuPDF\n",
    "    doc = fitz.open(f'{RAW_DATA_PATH}/{file_name}')\n",
    "    # Placeholder variable to track block index of every extracted text block\n",
    "    block_index = 0\n",
    "    # Placeholder variable to track page number of extracted text blocks\n",
    "    page_nr = 1\n",
    "\n",
    "    # Iterate over all pages of document\n",
    "    for page in doc:\n",
    "\n",
    "        # Extract all text blocks from the page\n",
    "        blocks = page.get_text('blocks')\n",
    "        \n",
    "        # Iterate over all text blocks\n",
    "        for block in blocks:\n",
    "\n",
    "            # The text data is in position 4 of the tuple\n",
    "            block_cleansed = block[4].replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "            # Only use blocks with a minimum length. This is a very naive approach. See description above this code cell\n",
    "            if len(block_cleansed) >= MIN_BLOCK_LENGTH:\n",
    "\n",
    "                # Append filename, page number, block index and the text as a row to our dataset\n",
    "                data.append([file_name, page_nr, block_index, block_cleansed])\n",
    "\n",
    "                block_index += 1 # Increment block index\n",
    "        page_nr += 1 # Increment page number\n",
    "\n",
    "# Put extracted texts into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['pdf_name', 'page_number', 'block_index', 'block_text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted text blocks are sometimes to big for the model to encode (paraphrase-multilingual-MiniLM-L12-v2) has a context length of 128 tokens. We therefore split up the text blocks into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>block_index</th>\n",
       "      <th>block_text</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau bedankt s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau bedankt s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3. Ihre elektronische Stellungnahme senden Sie...</td>\n",
       "      <td>0</td>\n",
       "      <td>3. Ihre elektronische Stellungnahme senden Sie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau begrüsst,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Der Regierungsrat des Kantons Aargau begrüsst,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Die \"Generaleinwilligung\" ist kein Freibrief f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Die \"Generaleinwilligung\" ist kein Freibrief f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Erlangt die Prüfperson nach Abschluss des klin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Erlangt die Prüfperson nach Abschluss des klin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pdf_name  page_number  \\\n",
       "0  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            1   \n",
       "1  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            2   \n",
       "2  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            4   \n",
       "3  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            4   \n",
       "4  fedlex-data-admin-ch-eli-dl-proj-2023-5-cons_1...            5   \n",
       "\n",
       "   block_index                                         block_text  \\\n",
       "0            0  Der Regierungsrat des Kantons Aargau bedankt s...   \n",
       "1            1  3. Ihre elektronische Stellungnahme senden Sie...   \n",
       "2            2  Der Regierungsrat des Kantons Aargau begrüsst,...   \n",
       "3            3  Die \"Generaleinwilligung\" ist kein Freibrief f...   \n",
       "4            4  Erlangt die Prüfperson nach Abschluss des klin...   \n",
       "\n",
       "   chunk_index                                         chunk_text  \n",
       "0            0  Der Regierungsrat des Kantons Aargau bedankt s...  \n",
       "1            0  3. Ihre elektronische Stellungnahme senden Sie...  \n",
       "2            0  Der Regierungsrat des Kantons Aargau begrüsst,...  \n",
       "3            0  Die \"Generaleinwilligung\" ist kein Freibrief f...  \n",
       "4            0  Erlangt die Prüfperson nach Abschluss des klin...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholder variable for text chunks\n",
    "data_chunks = []\n",
    "\n",
    "# Iterate over all text blocks\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    text = row['block_text']\n",
    "    pos = 0 # Pointer to iterate over text block\n",
    "    chunk_id = 0 # Placeholder for chunk_id per text block\n",
    "    \n",
    "    # As long as the pointer has not reached the end of the text block, keep generating chunks\n",
    "    while pos < len(text):\n",
    "        # At the beginning of a chunk we don't care about overlap\n",
    "        if pos == 0:\n",
    "            data_chunks.append(row.tolist() + [chunk_id, text[pos:pos+CHUNK_SIZE]])\n",
    "        # Chunks that start inside the text block have to have an overlap with the preceeding chunk\n",
    "        else:\n",
    "            data_chunks.append(row.tolist() + [chunk_id, text[pos-int(CHUNK_SIZE*OVERLAP):pos+CHUNK_SIZE]])\n",
    "        \n",
    "        pos += CHUNK_SIZE\n",
    "        chunk_id += 1\n",
    "\n",
    "    # Note: Chunking up the text using characters is a naive approach. Using the model's tokenizer would result in better results\n",
    "\n",
    "# Combine chunks with original data set\n",
    "df = df.merge(pd.DataFrame(data_chunks, columns=df.columns.tolist()+['chunk_index', 'chunk_text']), on=df.columns.tolist(), how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca373478275f484690cb7940144061d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings using the pretrained model from hugging face\n",
    "df['embeddings'] = model.encode(df['chunk_text'], show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in a parquet file for search in 01_search.ipynb\n",
    "df.to_parquet(f'{TRANSFORMED_DATA_PATH}/{output_file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_parldebatescanner_in_a_nutshell-jbN-841o",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
